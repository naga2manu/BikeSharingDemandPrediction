{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading and Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from csv file and load it into data frame called data\n",
    "data = pd.read_csv('./day.csv')\n",
    "\n",
    "# see the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see from above data 'instant' column is just like an index\n",
    "# so removing the 'instant' column as it will not contribute anythin to model\n",
    "data = data.drop(columns=['instant'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have the month and the Year in two seperate columns, we do not need the date column anymore, thus dropping it\n",
    "data.drop('dteday', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Model is to be built for <b>cnt</b> column, <b>casual</b> and <b>registed</b> are redundant here. It should not be used to build the model, thus dropping these two columns before further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as 'cnt' varibale is combination of causal and registerd and we are predicting the 'cnt'\n",
    "# so columns 'causal' and ''registerd' can be dropped 'cnt' is target variable\n",
    "data = data.drop(columns=['casual', 'registered'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data types of each column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the actual caterhorical variables from numerical to categorical\n",
    "cat_vars = ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "data[cat_vars] = data[cat_vars].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data types of each column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As from the above info we can see all columns doesnot have any missing values and all are non-null. so null value checking is not required in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the seasons variable values into their actual string values \n",
    "data['season'] = data['season'].replace([1, 2, 3, 4],['spring', 'summer', 'fall', 'winter'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the weather variable values into their actual string values\n",
    "data['weathersit'] = data['weathersit'].replace([1, 2, 3, 4],['Clear', 'Misty+Cloudy', 'Light Snow/Rain', 'Heavy Snow/Rain'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualising the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Numeric Variables\n",
    "\n",
    "Plot a pairplot of all the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking linear relationship between the cnt variable and other numeric variables\n",
    "num_vars = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "x = sns.pairplot(data, x_vars=num_vars, y_vars=['cnt'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see that cnt is having linear relationship with temp and atemp, where as humidity and windspeed is not having much linear and the data is spread across"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot season vs cnt variable \n",
    "sns.boxplot(x='season', y='cnt', data=data)\n",
    "plt.xlabel('Season')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that for summer and fall season the Count is more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Year vs cnt variable \n",
    "sns.boxplot(x='yr', y='cnt', data=data)\n",
    "plt.xlabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that for year 2019 Count is more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Month vs cnt variable \n",
    "sns.boxplot(x='mnth', y='cnt', data=data)\n",
    "plt.xlabel('Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that there is demand is increasing from March to September. So these months count will be more as they falls under summer and fall season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot holiday vs cnt variable \n",
    "sns.boxplot(x='holiday', y='cnt', data=data)\n",
    "plt.xlabel('Holiday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that holiday is not making any differnce in count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weekday vs cnt variable \n",
    "sns.boxplot(x='weekday', y='cnt', data=data)\n",
    "plt.xlabel('weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that Thursday and Friday is count is slightly more compare to other days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot workingday vs cnt variable \n",
    "sns.boxplot(x='workingday', y='cnt', data=data)\n",
    "plt.xlabel('workingday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that workingday or not is not making any differnce in count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weathersit vs cnt variable \n",
    "sns.boxplot(x='weathersit', y='cnt', data=data)\n",
    "plt.xlabel('weathersit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above plot it is observed that for clear weathersit day count is more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummpy variables for categorical variables\n",
    "dummy = pd.get_dummies(data[cat_vars], drop_first=True)\n",
    "dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge this dummy varibales to actual data frame and remove the categorical variables\n",
    "data = pd.concat([data, dummy], axis=1)   #Axis=1 is for horizontal stacking\n",
    "data = data.drop(columns=cat_vars, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Splitting the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# split data int0 70:30 \n",
    "df_train, df_test = train_test_split(data, train_size=0.7, test_size = 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data shape: ' , df_train.shape)\n",
    "print('test data shape: ' , df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Train Data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Test Data\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply min max scaling on the data\n",
    "\n",
    "#Instantiating scalar object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# get the columns of traing data frame\n",
    "cols = df_train.columns\n",
    "\n",
    "# fit and transform the data\n",
    "df_train[cols] = scaler.fit_transform(df_train[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the correlation coefficients to see which variables are highly correlated\n",
    "\n",
    "plt.figure(figsize = (19,10))\n",
    "sns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot temp and atemp is highly correlated to cnt compare to other variables.\n",
    "Lets plot pair plot of them with cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot temp vs cnt\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.scatter(df_train.temp, df_train.cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot atemp vs cnt\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.scatter(df_train.atemp, df_train.cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two plots temp and atemp having veru good linear relation ship with cnt variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Building a linear model\n",
    "Since the number of columns is 29, which is manageable, we first build a model with all the columns, and then keep removing the columns based upon Statistical Significance and Co-Linearity. <br>\n",
    "Here I am using Stats model to achieve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate x and y variables\n",
    "y_train = df_train.pop('cnt')\n",
    "X_train = df_train\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model1 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared is a significant 85%, but there are insignificant variables and variables with strong multicollinearity. We need to get rid of them, in the following cells, we will follow the same process in an itrative manner till we build a robust model. First we will remove all columns with High P Values and then when the P Values are acceptable for all the columns, we will check their VIF and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'weekday_4' which is having high P-Value of 0.869 as it is far greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('weekday_4', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model2 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'weekday_3' which is having high P-Value as it is greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('weekday_3', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model3 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'atemp' which is having high P-Value as it is greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('atemp', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model4 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'weekday_5' which is having high P-Value as it is greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('weekday_5', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model5 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'mnth_7' which is having high P-Value as it is greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('mnth_7', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object\n",
    "lr = sm.OLS(y_train, X_train_sm)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model6 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'mnth_11' which is having high P-Value as it is greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('mnth_11', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model7 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'mnth_12' which is having high P-Value as it is greater than parameter significance level of 5%\n",
    "X_train = X_train.drop('mnth_12', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model8 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'weekday_2' which is due to high P-Value\n",
    "X_train = X_train.drop('weekday_2', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit \n",
    "lr_model9 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'weekday_1' which is due to high P-Value\n",
    "X_train = X_train.drop('weekday_1', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit\n",
    "lr_model10 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'mnth_2' which due to high P-Value\n",
    "X_train = X_train.drop('mnth_2', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit\n",
    "lr_model11 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'season_summer' which due to high P-Value\n",
    "X_train = X_train.drop('season_summer', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit\n",
    "lr_model12 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model12.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'holiday_1' which due to high P-Value\n",
    "X_train = X_train.drop('holiday_1', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit\n",
    "lr_model13 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model13.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above model we can see that all the parameters p-values is less than the parameter siginificance level of 5%\n",
    "Now look into VIF of those parameters and remove any params is having more VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking VIF (Variance Inflation Factor - MultiColinearity)\n",
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humidity and Temperature have a high VIF, which means they have multicolinearity and one of them must be removed and checked again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing 'hum' due to high VIF\n",
    "X_train = X_train.drop('hum', axis=1)\n",
    "\n",
    "# add constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# instantiate the model object and fit\n",
    "lr_model14 = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# get the summary of the model\n",
    "lr_model14.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the VIF Again\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like an acceptable model. We keep the <b>temp</b> variable, because from our EDA, we have seen that Temperature has a direct colinearity with the booking count. On colder days, the bookings are less, whereas on hotter, summer time, the bookings are up significantly. Thus as per business understanding, we finalize this model as the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the co-efficients of the final model lr_model14\n",
    "print(lr_model14.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Residual Analysis of the train data\n",
    "\n",
    "So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lr_model14.predict(X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pred), bins = 20)\n",
    "fig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \n",
    "plt.xlabel('Errors', fontsize = 18)                         # X-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From the above plot t has been seen that residuals of training data is following normal distribution. Now make prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the assumptions of Linear Regression\n",
    "1. Linear Relationship\n",
    "2. No Multicollinearity\n",
    "3. Independence of residuals (absence of auto-correlation)\n",
    "4. Homoscedasticity- \n",
    "5. Normality of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Linear Relationship\n",
    "One of the most important assumptions is that a linear relationship is said to exist between the dependent and the \n",
    "independent variables. If you try to fit a linear relationship in a non-linear data set, the proposed algorithm won’t \n",
    "capture the trend as a linear graph, resulting in an inefficient model. Thus, it would result in inaccurate predictions.\n",
    "\n",
    "##### How can you determine if the assumption is met?\n",
    "\n",
    "The simple way to determine if this assumption is met or not is by creating a scatter plot x vs y. If the data points fall on a straight line in the graph, there is a linear relationship between the dependent and the independent variables, and the assumption holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Linear Relationship\n",
    "sm.graphics.plot_ccpr(lr_model14, 'temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial residual plot represents the relationship between the predictor and the dependent variable while taking into account all the other variables. As we can see in the above graph, the linearity is well respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No Multicollinearity\n",
    "\n",
    "The independent variables shouldn’t be correlated. If multicollinearity exists between the independent variables, \n",
    "it is challenging to predict the outcome of the model. In essence, it is difficult to explain the relationship between \n",
    "the dependent and the independent variables. In other words, it is unclear which independent variables explain the \n",
    "dependent variable.\n",
    "\n",
    "The standard errors tend to inflate with correlated variables, thus widening the confidence intervals leading to \n",
    "imprecise estimates.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Use a scatter plot to visualise the correlation between the variables. Another way is to determine the VIF\n",
    "(Variance Inflation Factor). VIF<=4 implies no multicollinearity, whereas VIF>=10 implies serious multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Multi Colinearity\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(X_train.corr(), annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables have less than 0.56 correlation with eachother. Checking the VIF now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking 10 as the maximum VIF permissible for this model, we decide on keeping these colmns based upon business assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. No auto-correlation or independence\n",
    "The residuals (error terms) are independent of each other. In other words, there is no correlation between the \n",
    "consecutive error terms of the time series data. The presence of correlation in the error terms drastically \n",
    "reduces the accuracy of the model. If the error terms are correlated, the estimated standard error tries to \n",
    "deflate the true standard error.\n",
    "\n",
    "###### How to determine if the assumption is met?\n",
    "\n",
    "Conduct a Durbin-Watson (DW) statistic test. The values should fall between 0-4. If DW=2, no auto-correlation; \n",
    "if DW lies between 0 and 2, it means that there exists a positive correlation. If DW lies between 2 and 4, \n",
    "it means there is a negative correlation. Another method is to plot a graph against residuals vs time and see \n",
    "patterns in residual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of residuals (absence of auto-correlation)\n",
    "# Autocorrelation refers to the fact that observations’ errors are correlated\n",
    "# To verify that the observations are not auto-correlated, we can use the Durbin-Watson test. \n",
    "# The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables\n",
    "# (0–2: positive auto-correlation, 2–4: negative auto-correlation)\n",
    "\n",
    "print('The Durbin-Watson value for Model No.19 is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost nill auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homoscedasticity\n",
    "Homoscedasticity means the residuals have constant variance at every level of x. The absence of this phenomenon is \n",
    "known as heteroscedasticity. Heteroscedasticity generally arises in the presence of outliers and extreme values.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "Create a scatter plot that shows residual vs fitted value. If the data points are spread across equally without a \n",
    "prominent pattern, it means the residuals have constant variance (homoscedasticity). Otherwise, if a funnel-shaped \n",
    "pattern is seen, it means the residuals are not distributed equally and depicts a non-constant variance (heteroscedasticity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Homoscedasticity : The residuals have constant variance with respect to the dependent variable\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "sns.scatterplot(y_train,(y_train - y_train_pred))\n",
    "plt.plot(y_train,(y_train - y_train), '-r')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, Homoscedasticity is well respected since the variance of the residuals are almost constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal distribution of error terms\n",
    "The last assumption that needs to be checked for linear regression is the error terms’ normal distribution. \n",
    "If the error terms don’t follow a normal distribution, confidence intervals may become too wide or narrow.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Check the assumption using a Q-Q (Quantile-Quantile) plot. If the data points on the graph form a straight diagonal line, \n",
    "the assumption is met.\n",
    "You can also check for the error terms’ normality using statistical tests like the Kolmogorov-Smironov or Shapiro-Wilk test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of Errors\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "\n",
    "# Ploting the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pred))\n",
    "fig.suptitle('Error Terms')                  \n",
    "plt.xlabel('Errors')     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot((y_train - y_train_pred), fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error terms are normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Making Predictions Using the Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Test Dataset with the Scaler of the Training Set\n",
    "cols = df_test.columns\n",
    "df_test[cols] = scaler.transform(df_test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the transformed data\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data into X_test and y_test\n",
    "y_test = df_test.pop('cnt')\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as in the training data some of the non-significant columns is removed for model building.\n",
    "# so for test data also we have to remove those columns or filter data only for significant columns\n",
    "# get the training data final columns\n",
    "final_columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data\n",
    "X_test[final_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub set the test data based on significant columns which is acquired from training data\n",
    "X_test_m14 = X_test[final_columns]\n",
    "# Adding the constant column\n",
    "X_test_m14 = sm.add_constant(X_test_m14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction using Model 14\n",
    "y_test_pred = lr_model14.predict(X_test_m14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Evaluation\n",
    "\n",
    "Let's now plot the graph for actual versus predicted values of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "fig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \n",
    "plt.xlabel('y_test', fontsize = 18)                          # X-label\n",
    "plt.ylabel('y_test_pred', fontsize = 16)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see actual and predicted are aligned each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets check the r2 and adj-r2 for training and testing data\n",
    "r2_train = r2_score(y_true=y_train, y_pred=y_train_pred)\n",
    "r2_test = r2_score(y_true=y_test, y_pred=y_test_pred)\n",
    "print(\"Training data r2-value : \", r2_train)\n",
    "print(\"Testing data r2-value : \", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see above r2 value for training and testing is near to each other. So we can tell that model is performing good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the best fitted line developed by Model 14 is:\n",
    "\n",
    "$ 𝑐𝑛𝑡=0.1693+(0.4121*temp-0.1544*windspeed-0.0759*season_spring+0.0833*season_winter+0.2352*yr_1+0.0620*mnth_3+0.0705*mnth_4+0.0880*mnth_5+0.0633*mnth_6+0.0512*mnth_8+0.1124*mnth_9+0.0500*mnth_10+0.0651*weekday_6+0.0533*workingday_1-0.2968*weathersit_Light Snow/Rain-0.0838*weathersit_Misty+Cloudy) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the bookings increase on Clear weather days with hotter temperature, the company must increase their bike availibilty and promotions during the summer months to further increase their booking count.\n",
    "\n",
    "An R-Squared value of 0.82 on the test data signifies that the model is a very good predictor and 82% of the variance is captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a linear model using RFE(Recursive feature elimination) and sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the above steps i have build the model using manually removing the insignificat varaibles one by one at a time.\n",
    "Now in below I am building the model using automatic way of removing the insigificant parameters using RFE Method\n",
    "and build the model using that and see both are giving same kind of accuracy or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing into X and Y sets for the model building\n",
    "y_train = y_train\n",
    "X_train = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as from the manual mode finally we got 16parameters as significant so I am also using same count for RFE model\n",
    "# Running RFE with the output number of the variable equal to 16\n",
    "# create linear regression model object and fit train and test data\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# create rfe object\n",
    "rfe = RFE(lm, 16)  \n",
    "# fit the model\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameter ranks\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the only significant params\n",
    "col = X_train.columns[rfe.support_]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print insignificant params\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model using statsmodel, for the detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_test dataframe with RFE selected variables\n",
    "X_train_rfe = X_train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant variable \n",
    "import statsmodels.api as sm  \n",
    "X_train_rfe = sm.add_constant(X_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the linear model\n",
    "lm = sm.OLS(y_train, X_train_rfe).fit()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our linear model\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the VIFs for the new model\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X = X_train_rfe\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above VIF table everything is under 5 so all the parameters are siginificant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis of the train data\n",
    "\n",
    "So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lm.predict(X_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pred), bins = 20)\n",
    "fig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \n",
    "plt.xlabel('Errors', fontsize = 18)                         # X-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot its clear that resisuals is following Normal Distribution. So model is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the assumptions of Linear Regression\n",
    "1. Linear Relationship\n",
    "2. No Multicollinearity\n",
    "3. Independence of residuals (absence of auto-correlation)\n",
    "4. Homoscedasticity- \n",
    "5. Normality of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Linear Relationship\n",
    "One of the most important assumptions is that a linear relationship is said to exist between the dependent and the \n",
    "independent variables. If you try to fit a linear relationship in a non-linear data set, the proposed algorithm won’t \n",
    "capture the trend as a linear graph, resulting in an inefficient model. Thus, it would result in inaccurate predictions.\n",
    "\n",
    "##### How can you determine if the assumption is met?\n",
    "\n",
    "The simple way to determine if this assumption is met or not is by creating a scatter plot x vs y. If the data points fall on a straight line in the graph, there is a linear relationship between the dependent and the independent variables, and the assumption holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Linear Relationship\n",
    "sm.graphics.plot_ccpr(lr_model14, 'temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial residual plot represents the relationship between the predictor and the dependent variable while taking into account all the other variables. As we can see in the above graph, the linearity is well respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No Multicollinearity\n",
    "\n",
    "The independent variables shouldn’t be correlated. If multicollinearity exists between the independent variables, \n",
    "it is challenging to predict the outcome of the model. In essence, it is difficult to explain the relationship between \n",
    "the dependent and the independent variables. In other words, it is unclear which independent variables explain the \n",
    "dependent variable.\n",
    "\n",
    "The standard errors tend to inflate with correlated variables, thus widening the confidence intervals leading to \n",
    "imprecise estimates.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Use a scatter plot to visualise the correlation between the variables. Another way is to determine the VIF\n",
    "(Variance Inflation Factor). VIF<=4 implies no multicollinearity, whereas VIF>=10 implies serious multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Multi Colinearity\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(X_train.corr(), annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables have less than 0.56 correlation with eachother. Checking the VIF now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking 10 as the maximum VIF permissible for this model, we decide on keeping these colmns based upon business assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. No auto-correlation or independence\n",
    "The residuals (error terms) are independent of each other. In other words, there is no correlation between the \n",
    "consecutive error terms of the time series data. The presence of correlation in the error terms drastically \n",
    "reduces the accuracy of the model. If the error terms are correlated, the estimated standard error tries to \n",
    "deflate the true standard error.\n",
    "\n",
    "###### How to determine if the assumption is met?\n",
    "\n",
    "Conduct a Durbin-Watson (DW) statistic test. The values should fall between 0-4. If DW=2, no auto-correlation; \n",
    "if DW lies between 0 and 2, it means that there exists a positive correlation. If DW lies between 2 and 4, \n",
    "it means there is a negative correlation. Another method is to plot a graph against residuals vs time and see \n",
    "patterns in residual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of residuals (absence of auto-correlation)\n",
    "# Autocorrelation refers to the fact that observations’ errors are correlated\n",
    "# To verify that the observations are not auto-correlated, we can use the Durbin-Watson test. \n",
    "# The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables\n",
    "# (0–2: positive auto-correlation, 2–4: negative auto-correlation)\n",
    "\n",
    "print('The Durbin-Watson value for Model No.19 is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost nill auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homoscedasticity\n",
    "Homoscedasticity means the residuals have constant variance at every level of x. The absence of this phenomenon is \n",
    "known as heteroscedasticity. Heteroscedasticity generally arises in the presence of outliers and extreme values.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "Create a scatter plot that shows residual vs fitted value. If the data points are spread across equally without a \n",
    "prominent pattern, it means the residuals have constant variance (homoscedasticity). Otherwise, if a funnel-shaped \n",
    "pattern is seen, it means the residuals are not distributed equally and depicts a non-constant variance (heteroscedasticity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Homoscedasticity : The residuals have constant variance with respect to the dependent variable\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "sns.scatterplot(y_train,(y_train - y_train_pred))\n",
    "plt.plot(y_train,(y_train - y_train), '-r')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, Homoscedasticity is well respected since the variance of the residuals are almost constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal distribution of error terms\n",
    "The last assumption that needs to be checked for linear regression is the error terms’ normal distribution. \n",
    "If the error terms don’t follow a normal distribution, confidence intervals may become too wide or narrow.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Check the assumption using a Q-Q (Quantile-Quantile) plot. If the data points on the graph form a straight diagonal line, \n",
    "the assumption is met.\n",
    "You can also check for the error terms’ normality using statistical tests like the Kolmogorov-Smironov or Shapiro-Wilk test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of Errors\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "\n",
    "# Ploting the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pred))\n",
    "fig.suptitle('Error Terms')                  \n",
    "plt.xlabel('Errors')     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot((y_train - y_train_pred), fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error terms are normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions Using the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the transformed data\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data into X_test and y_test\n",
    "y_test = y_test\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as in the training data some of the non-significant columns is removed for model building.\n",
    "# so for test data also we have to remove those columns or filter data only for significant columns\n",
    "# get the training data final columns\n",
    "final_columns = list(X_train_rfe.columns)\n",
    "final_columns.remove(\"const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data\n",
    "X_train_rfe[final_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub set the test data based on significant columns which is acquired from training data\n",
    "X_test_rfe = X_test[final_columns]\n",
    "# Adding the constant column\n",
    "X_test_rfe = sm.add_constant(X_test_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction using Model\n",
    "y_test_pred = lm.predict(X_test_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's now plot the graph for actual versus predicted values of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "fig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \n",
    "plt.xlabel('y_test', fontsize = 18)                          # X-label\n",
    "plt.ylabel('y_test_pred', fontsize = 16)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see actual and predicted are aligned each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets check the r2 and adj-r2 for training and testing data\n",
    "r2_train = r2_score(y_true=y_train, y_pred=y_train_pred)\n",
    "r2_test = r2_score(y_true=y_test, y_pred=y_test_pred)\n",
    "print(\"Training data r2-value : \", r2_train)\n",
    "print(\"Testing data r2-value : \", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see above r2 value for training and testing is near to each other. So we can tell that model is performing good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the best fitted line developed by Model 14 is:\n",
    "\n",
    "$ 𝑐𝑛𝑡=0.3031+(0.4385*temp-0.1620*hum-0.1823*windspeed-0.0699*season_spring+0.0949*season_winter+0.2313*yr_1+0.0628*mnth_3+0.0703*mnth_4+0.0983*mnth_5+0.0586*mnth_6+0.0548*mnth_8+0.1221*mnth_9+0.0488*mnth_10-0.0859*holiday_1-0.2481*weathersit_Light Snow/Rain-0.0567*weathersit_Misty+Cloudy) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two models we build one using manual elimination of insginificant parameters and other using RFE method of\n",
    "eliminating insginificant parameters it is observed that both models is having same accuracy of r2 values for traing and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the bookings increase on Clear weather days with hotter temperature, the company must increase their bike availibilty and promotions during the summer months to further increase their booking count.\n",
    "\n",
    "An R-Squared value of 0.82 on the test data signifies that the model is a very good predictor and 82% of the variance is captured by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the assumptions of Linear Regression\n",
    "1. Linear Relationship\n",
    "2. No Multicollinearity\n",
    "3. Independence of residuals (absence of auto-correlation)\n",
    "4. Homoscedasticity- \n",
    "5. Normality of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Linear Relationship\n",
    "One of the most important assumptions is that a linear relationship is said to exist between the dependent and the \n",
    "independent variables. If you try to fit a linear relationship in a non-linear data set, the proposed algorithm won’t \n",
    "capture the trend as a linear graph, resulting in an inefficient model. Thus, it would result in inaccurate predictions.\n",
    "\n",
    "##### How can you determine if the assumption is met?\n",
    "\n",
    "The simple way to determine if this assumption is met or not is by creating a scatter plot x vs y. If the data points fall on a straight line in the graph, there is a linear relationship between the dependent and the independent variables, and the assumption holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Linear Relationship\n",
    "sm.graphics.plot_ccpr(lr_model14, 'temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial residual plot represents the relationship between the predictor and the dependent variable while taking into account all the other variables. As we can see in the above graph, the linearity is well respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No Multicollinearity\n",
    "\n",
    "The independent variables shouldn’t be correlated. If multicollinearity exists between the independent variables, \n",
    "it is challenging to predict the outcome of the model. In essence, it is difficult to explain the relationship between \n",
    "the dependent and the independent variables. In other words, it is unclear which independent variables explain the \n",
    "dependent variable.\n",
    "\n",
    "The standard errors tend to inflate with correlated variables, thus widening the confidence intervals leading to \n",
    "imprecise estimates.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Use a scatter plot to visualise the correlation between the variables. Another way is to determine the VIF\n",
    "(Variance Inflation Factor). VIF<=4 implies no multicollinearity, whereas VIF>=10 implies serious multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Multi Colinearity\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(X_train.corr(), annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables have less than 0.56 correlation with eachother. Checking the VIF now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking 10 as the maximum VIF permissible for this model, we decide on keeping these colmns based upon business assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. No auto-correlation or independence\n",
    "The residuals (error terms) are independent of each other. In other words, there is no correlation between the \n",
    "consecutive error terms of the time series data. The presence of correlation in the error terms drastically \n",
    "reduces the accuracy of the model. If the error terms are correlated, the estimated standard error tries to \n",
    "deflate the true standard error.\n",
    "\n",
    "###### How to determine if the assumption is met?\n",
    "\n",
    "Conduct a Durbin-Watson (DW) statistic test. The values should fall between 0-4. If DW=2, no auto-correlation; \n",
    "if DW lies between 0 and 2, it means that there exists a positive correlation. If DW lies between 2 and 4, \n",
    "it means there is a negative correlation. Another method is to plot a graph against residuals vs time and see \n",
    "patterns in residual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of residuals (absence of auto-correlation)\n",
    "# Autocorrelation refers to the fact that observations’ errors are correlated\n",
    "# To verify that the observations are not auto-correlated, we can use the Durbin-Watson test. \n",
    "# The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables\n",
    "# (0–2: positive auto-correlation, 2–4: negative auto-correlation)\n",
    "\n",
    "print('The Durbin-Watson value for Model No.19 is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost nill auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homoscedasticity\n",
    "Homoscedasticity means the residuals have constant variance at every level of x. The absence of this phenomenon is \n",
    "known as heteroscedasticity. Heteroscedasticity generally arises in the presence of outliers and extreme values.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "Create a scatter plot that shows residual vs fitted value. If the data points are spread across equally without a \n",
    "prominent pattern, it means the residuals have constant variance (homoscedasticity). Otherwise, if a funnel-shaped \n",
    "pattern is seen, it means the residuals are not distributed equally and depicts a non-constant variance (heteroscedasticity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Homoscedasticity : The residuals have constant variance with respect to the dependent variable\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "sns.scatterplot(y_train,(y_train - y_train_pred))\n",
    "plt.plot(y_train,(y_train - y_train), '-r')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, Homoscedasticity is well respected since the variance of the residuals are almost constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal distribution of error terms\n",
    "The last assumption that needs to be checked for linear regression is the error terms’ normal distribution. \n",
    "If the error terms don’t follow a normal distribution, confidence intervals may become too wide or narrow.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Check the assumption using a Q-Q (Quantile-Quantile) plot. If the data points on the graph form a straight diagonal line, \n",
    "the assumption is met.\n",
    "You can also check for the error terms’ normality using statistical tests like the Kolmogorov-Smironov or Shapiro-Wilk test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of Errors\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "\n",
    "# Ploting the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pred))\n",
    "fig.suptitle('Error Terms')                  \n",
    "plt.xlabel('Errors')     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot((y_train - y_train_pred), fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error terms are normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the assumptions of Linear Regression\n",
    "1. Linear Relationship\n",
    "2. No Multicollinearity\n",
    "3. Independence of residuals (absence of auto-correlation)\n",
    "4. Homoscedasticity- \n",
    "5. Normality of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Linear Relationship\n",
    "One of the most important assumptions is that a linear relationship is said to exist between the dependent and the \n",
    "independent variables. If you try to fit a linear relationship in a non-linear data set, the proposed algorithm won’t \n",
    "capture the trend as a linear graph, resulting in an inefficient model. Thus, it would result in inaccurate predictions.\n",
    "\n",
    "##### How can you determine if the assumption is met?\n",
    "\n",
    "The simple way to determine if this assumption is met or not is by creating a scatter plot x vs y. If the data points fall on a straight line in the graph, there is a linear relationship between the dependent and the independent variables, and the assumption holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Linear Relationship\n",
    "sm.graphics.plot_ccpr(lr_model14, 'temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial residual plot represents the relationship between the predictor and the dependent variable while taking into account all the other variables. As we can see in the above graph, the linearity is well respected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. No Multicollinearity\n",
    "\n",
    "The independent variables shouldn’t be correlated. If multicollinearity exists between the independent variables, \n",
    "it is challenging to predict the outcome of the model. In essence, it is difficult to explain the relationship between \n",
    "the dependent and the independent variables. In other words, it is unclear which independent variables explain the \n",
    "dependent variable.\n",
    "\n",
    "The standard errors tend to inflate with correlated variables, thus widening the confidence intervals leading to \n",
    "imprecise estimates.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Use a scatter plot to visualise the correlation between the variables. Another way is to determine the VIF\n",
    "(Variance Inflation Factor). VIF<=4 implies no multicollinearity, whereas VIF>=10 implies serious multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Multi Colinearity\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(X_train.corr(), annot=True, cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables have less than 0.56 correlation with eachother. Checking the VIF now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking 10 as the maximum VIF permissible for this model, we decide on keeping these colmns based upon business assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. No auto-correlation or independence\n",
    "The residuals (error terms) are independent of each other. In other words, there is no correlation between the \n",
    "consecutive error terms of the time series data. The presence of correlation in the error terms drastically \n",
    "reduces the accuracy of the model. If the error terms are correlated, the estimated standard error tries to \n",
    "deflate the true standard error.\n",
    "\n",
    "###### How to determine if the assumption is met?\n",
    "\n",
    "Conduct a Durbin-Watson (DW) statistic test. The values should fall between 0-4. If DW=2, no auto-correlation; \n",
    "if DW lies between 0 and 2, it means that there exists a positive correlation. If DW lies between 2 and 4, \n",
    "it means there is a negative correlation. Another method is to plot a graph against residuals vs time and see \n",
    "patterns in residual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of residuals (absence of auto-correlation)\n",
    "# Autocorrelation refers to the fact that observations’ errors are correlated\n",
    "# To verify that the observations are not auto-correlated, we can use the Durbin-Watson test. \n",
    "# The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables\n",
    "# (0–2: positive auto-correlation, 2–4: negative auto-correlation)\n",
    "\n",
    "print('The Durbin-Watson value for Model No.19 is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost nill auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homoscedasticity\n",
    "Homoscedasticity means the residuals have constant variance at every level of x. The absence of this phenomenon is \n",
    "known as heteroscedasticity. Heteroscedasticity generally arises in the presence of outliers and extreme values.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "Create a scatter plot that shows residual vs fitted value. If the data points are spread across equally without a \n",
    "prominent pattern, it means the residuals have constant variance (homoscedasticity). Otherwise, if a funnel-shaped \n",
    "pattern is seen, it means the residuals are not distributed equally and depicts a non-constant variance (heteroscedasticity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Homoscedasticity : The residuals have constant variance with respect to the dependent variable\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "sns.scatterplot(y_train,(y_train - y_train_pred))\n",
    "plt.plot(y_train,(y_train - y_train), '-r')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, Homoscedasticity is well respected since the variance of the residuals are almost constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normal distribution of error terms\n",
    "The last assumption that needs to be checked for linear regression is the error terms’ normal distribution. \n",
    "If the error terms don’t follow a normal distribution, confidence intervals may become too wide or narrow.\n",
    "\n",
    "##### How to determine if the assumption is met?\n",
    "\n",
    "Check the assumption using a Q-Q (Quantile-Quantile) plot. If the data points on the graph form a straight diagonal line, \n",
    "the assumption is met.\n",
    "You can also check for the error terms’ normality using statistical tests like the Kolmogorov-Smironov or Shapiro-Wilk test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of Errors\n",
    "y_train_pred = lr_model14.predict(X_train_sm)\n",
    "\n",
    "# Ploting the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pred))\n",
    "fig.suptitle('Error Terms')                  \n",
    "plt.xlabel('Errors')     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot((y_train - y_train_pred), fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error terms are normally distributed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
